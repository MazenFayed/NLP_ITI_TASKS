{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "8Lu5JtKOu4A1"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIwveUWuU221"
      },
      "source": [
        "#Create config file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bLmEFM_RunBs",
        "outputId": "2a86114d-5704-4808-d446-fac3864eb412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[38;5;1m✘ The provided output file already exists. To force overwriting the\n",
            "config file, set the --force or -F flag.\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy init config config.cfg --lang en --pipeline ner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI5FiyGmv2mz",
        "outputId": "e4d5023d-a660-4867-84c3-0dee010263ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COBZmmjoUyqF"
      },
      "source": [
        "#Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE5FHvIESQDG",
        "outputId": "ea947730-3c2b-4714-bcb9-bc84a62af092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
            "        num_rows: 14041\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
            "        num_rows: 3250\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
            "        num_rows: 3453\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the CoNLL-03 dataset\n",
        "dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "# Check the dataset\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxuQw5sSUt7o"
      },
      "source": [
        "# Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DiMtzCGzy1p",
        "outputId": "13584d48-db99-492d-aa44-9b451c5b2c5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = dataset['train']\n",
        "train_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7ZPlyIUUoxl"
      },
      "source": [
        "#Convert datasets to spacy data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQGS2zCNO1TC",
        "outputId": "12a90e7f-9758-4900-9522-6f91e7a73d49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EU rejects German call to boycott British lamb .\n",
            "[('EU', 'LOC'), ('German', 'UNKNOWN'), ('British', 'UNKNOWN')]\n"
          ]
        }
      ],
      "source": [
        "from spacy.training.example import Example\n",
        "from spacy.tokens import DocBin\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load a blank spaCy model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Define the NER labels\n",
        "ner_labels = {\n",
        "    0: \"O\",    # No entity\n",
        "    1: \"PER\",\n",
        "    2: \"ORG\",\n",
        "    3: \"LOC\",\n",
        "    4: \"MISC\",\n",
        "    # Add other labels if necessary\n",
        "}\n",
        "\n",
        "# Convert the CoNLL-style dataset to spaCy format\n",
        "def convert_to_spacy_format(example):\n",
        "    tokens = example['tokens']\n",
        "    ner_tags = example['ner_tags']\n",
        "\n",
        "    # Recreate the full text from tokens\n",
        "    text = \" \".join(tokens)\n",
        "    doc = nlp.make_doc(text)\n",
        "    entities = []\n",
        "\n",
        "    # get correct character offsets\n",
        "    char_idx = 0\n",
        "    for token, tag in zip(tokens, ner_tags):\n",
        "        start = text.find(token, char_idx)\n",
        "        end = start + len(token)\n",
        "        char_idx = end  # update for next token\n",
        "\n",
        "        if tag != 0:\n",
        "            label = ner_labels.get(tag, \"UNKNOWN\")\n",
        "            entities.append((start, end, label))\n",
        "\n",
        "    # Create character-based entity spans\n",
        "    spans = []\n",
        "    for start, end, label in entities:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "        if span is not None:\n",
        "            spans.append(span)\n",
        "\n",
        "    doc.ents = spans\n",
        "    return Example.from_dict(doc, {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in spans]})\n",
        "\n",
        "# Load the dataset\n",
        "train_data = dataset['train']\n",
        "\n",
        "# Convert the dataset\n",
        "spacy_train_data = [convert_to_spacy_format(example) for example in train_data]\n",
        "\n",
        "# Test output\n",
        "print(spacy_train_data[0].text)\n",
        "print([(ent.text, ent.label_) for ent in spacy_train_data[0].reference.ents])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl9usLED2aRH"
      },
      "outputs": [],
      "source": [
        "validation_data = dataset['validation']\n",
        "\n",
        "# Convert the dataset to spaCy format\n",
        "spacy_validation_data = [convert_to_spacy_format(example) for example in validation_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fcrm4v224mTE"
      },
      "outputs": [],
      "source": [
        "test_data = dataset['test']\n",
        "\n",
        "# Convert the dataset to spaCy format\n",
        "spacy_test_data = [convert_to_spacy_format(example) for example in test_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTLOkc1TUkRx"
      },
      "source": [
        "#Save datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9h46QQQ40Tx",
        "outputId": "61de43eb-35fb-405a-d3e5-aa6236a3db34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved train.spacy\n",
            "Saved dev.spacy\n",
            "Saved test.spacy\n"
          ]
        }
      ],
      "source": [
        "def save_spacy_file(examples, filename):\n",
        "    doc_bin = DocBin()\n",
        "    for example in examples:\n",
        "        doc_bin.add(example.reference)  # reference is the annotated Doc\n",
        "    doc_bin.to_disk(filename)\n",
        "    print(f\"Saved {filename}\")\n",
        "\n",
        "# Save train, validation, and test data\n",
        "save_spacy_file(spacy_train_data, \"train.spacy\")\n",
        "save_spacy_file(spacy_validation_data, \"dev.spacy\")\n",
        "save_spacy_file(spacy_test_data, \"test.spacy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6Pj0RXxUfZO"
      },
      "source": [
        "#Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xop76AkC5LRO",
        "outputId": "e5cf1ebe-5263-4152-cf31-78357795598d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     47.05    0.00    0.00    0.00    0.00\n",
            "  0     200         92.85   3283.77   63.95   67.19   61.01    0.64\n",
            "  0     400        171.60   2060.99   76.86   78.53   75.25    0.77\n",
            "  0     600        227.44   1882.72   80.25   82.12   78.46    0.80\n",
            "  0     800        296.25   2005.03   84.24   85.54   82.98    0.84\n",
            "  0    1000        294.57   2105.42   85.33   86.03   84.64    0.85\n",
            "  1    1200        327.41   2030.03   86.86   88.00   85.75    0.87\n",
            "  1    1400        443.31   1525.37   88.25   88.66   87.83    0.88\n",
            "  1    1600        404.12   1721.07   88.28   89.04   87.54    0.88\n",
            "  2    1800        476.88   1749.80   88.83   89.84   87.85    0.89\n",
            "  2    2000        513.51   1488.12   89.24   90.79   87.75    0.89\n",
            "  3    2200        522.49   1241.14   89.73   90.03   89.43    0.90\n",
            "  4    2400        594.80   1165.15   89.63   90.39   88.88    0.90\n",
            "  5    2600        623.18   1013.70   89.55   90.41   88.70    0.90\n",
            "  6    2800        553.67    747.62   89.70   90.04   89.38    0.90\n",
            "  7    3000        620.44    792.76   89.22   90.20   88.26    0.89\n",
            "  8    3200        594.74    667.96   89.20   89.92   88.48    0.89\n",
            "  9    3400        595.80    588.68   89.37   90.24   88.52    0.89\n",
            "  9    3600        621.27    567.17   89.49   90.12   88.88    0.89\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./dev.spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnvb1Wd7VDUa"
      },
      "source": [
        "#Evaluate the model on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTM_vTba8tFo",
        "outputId": "4f39d51b-bb05-4ece-9bb2-09ace3559de0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     100.00\n",
            "NER P   81.62 \n",
            "NER R   84.05 \n",
            "NER F   82.82 \n",
            "SPEED   12923 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "              P       R       F\n",
            "UNKNOWN   78.14   87.65   82.63\n",
            "PER       85.62   81.01   83.25\n",
            "LOC       81.97   75.02   78.34\n",
            "ORG       90.38   94.29   92.29\n",
            "MISC      74.89   81.44   78.03\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy evaluate ./output/model-best ./test.spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fEbrcW_VJnh"
      },
      "source": [
        "# Test the model on random sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zA1DNUJN55c"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"./output/model-best\")\n",
        "\n",
        "# Test sentence\n",
        "text = \"U.N. official Ekeus heads for Baghdad.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print entities\n",
        "print(\"Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "znS6yysz-Or1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
